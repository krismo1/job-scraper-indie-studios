"""
Scraper de ArtStation con Playwright + Supabase
Proyecto: Sistema de B√∫squeda Indie/Outsourcing
Autor: Cristian Meza Venegas

VERSI√ìN INTEGRADA CON PERSISTENCIA EN SUPABASE
"""

import sys
import os
import json
import time
import re
from datetime import datetime

# Agregar carpeta ra√≠z al path para imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from scraper_base import PlaywrightScraper

# Imports de base de datos
try:
    from models import (
        SessionLocal,
        save_job,
        start_job_run,
        finish_job_run,
    )

    DB_AVAILABLE = True
    print("‚úì M√≥dulo de base de datos cargado correctamente")
except ImportError as e:
    DB_AVAILABLE = False
    print(f"‚ö†Ô∏è  Base de datos no disponible: {e}")
    print("   El scraper funcionar√° pero NO guardar√° en Supabase")


class ArtStationScraper(PlaywrightScraper):
    """Scraper espec√≠fico para ArtStation Jobs con filtros Character Artist"""

    def __init__(self, headless: bool = False):
        """
        Inicializar scraper de ArtStation

        Args:
            headless: False para VER el navegador (√∫til para debugging)
        """
        super().__init__(headless=headless, delay=2)
        self.base_url = "https://www.artstation.com/jobs"

    def _is_character_artist(self, title: str) -> bool:
        """
        Determinar si el trabajo es para Character Artist

        Args:
            title: T√≠tulo del trabajo

        Returns:
            True si es posici√≥n de character artist
        """
        title_lower = title.lower()

        # Keywords principales (alta prioridad)
        character_keywords = [
            'character artist',
            'character modeler',
            'character designer',
            '3d character',
            'character animator',
            'character sculptor',
            'character rigger',
        ]

        # Keywords secundarias (prioridad media - tambi√©n relevantes)
        related_keywords = [
            '3d artist',
            '3d generalist',
            'game artist',
            'asset artist',
            'organic modeler',
        ]

        # Revisar keywords principales
        for keyword in character_keywords:
            if keyword in title_lower:
                return True

        # Revisar keywords secundarias
        for keyword in related_keywords:
            if keyword in title_lower:
                return True

        return False

    def _is_entry_level(self, title: str, description: str = '') -> bool:
        """
        Determinar si es posici√≥n entry-level o junior

        Args:
            title: T√≠tulo del trabajo
            description: Descripci√≥n o informaci√≥n adicional

        Returns:
            True si es entry level
        """
        text_lower = f"{title} {description}".lower()

        # Indicators de entry level
        entry_keywords = [
            'junior',
            'entry level',
            'entry-level',
            'associate',
            'graduate',
            'intern',
            'trainee',
            '0-2 years',
            '1-2 years',
            'early career',
            'less than 2 years',
        ]

        for keyword in entry_keywords:
            if keyword in text_lower:
                return True

        # Excluir expl√≠citamente si menciona senior/lead
        senior_keywords = [
            'senior',
            'lead',
            'principal',
            'director',
            'head of',
            '5+ years',
            '3+ years',
            'experienced',
        ]

        for keyword in senior_keywords:
            if keyword in text_lower:
                return False

        # Si no menciona nivel espec√≠fico, asumir que PODR√çA ser entry
        return True

    def _extract_experience_years(self, description: str) -> str:
        """
        Extraer a√±os de experiencia requeridos

        Args:
            description: Texto de descripci√≥n

        Returns:
            String con a√±os encontrados o 'Not specified'
        """
        patterns = [
            r'(\d+)\+?\s*years?',
            r'(\d+)-(\d+)\s*years?',
        ]

        for pattern in patterns:
            match = re.search(pattern, description.lower())
            if match:
                return match.group(0)

        return 'Not specified'

    def _calculate_relevance(self, is_character: bool, is_entry: bool) -> int:
        """
        Calcular score de relevancia (0-10)

        Args:
            is_character: Si es posici√≥n de character artist
            is_entry: Si es entry level

        Returns:
            Score de 0-10
        """
        score = 0

        if is_character:
            score += 6  # Base para character artist

        if is_entry:
            score += 4  # Bonus para entry level

        return score

    def _to_db_job(self, raw_job: dict) -> dict:
        """
        Convertir job scrapeado al formato del modelo Job (Supabase)

        Args:
            raw_job: Diccionario con datos scrapeados

        Returns:
            Diccionario con formato compatible con modelo Job
        """
        return {
            "platform": "ArtStation",
            "external_id": None,
            "url": raw_job.get("url"),
            "title": raw_job.get("title"),
            "company": raw_job.get("company"),
            "location": raw_job.get("location_info"),
            "remote_type": (
                "Remote"
                if "remote" in (raw_job.get("location_info") or "").lower()
                else None
            ),
            "description": raw_job.get("description"),
            "company_size": None,
            "company_type": None,
            "is_character_artist": raw_job.get("is_character_artist"),
            "is_entry_level": raw_job.get("is_entry_level"),
            "relevance_score": raw_job.get("relevance_score"),
            "posted_date": None
        }

    def scrape_job_detail(self, raw_job: dict) -> dict:
        """
        FASE DETAIL: Entra al job individual y extrae descripci√≥n completa

        Args:
            raw_job: Job base con URL

        Returns:
            Job enriquecido con descripci√≥n
        """
        job_url = raw_job.get("url")

        if not job_url or not job_url.startswith("http"):
            print("    ‚ö†Ô∏è  URL inv√°lida, se omite DETAIL")
            return raw_job

        print(f"    ‚Üí Visitando detalle: {job_url[:60]}...")

        success = self.navigate_to(job_url)
        if not success:
            print("    ‚úó No se pudo cargar el detalle")
            return raw_job

        time.sleep(2)

        html = self.get_html()
        soup = self.parse_html(html)

        # ===== DESCRIPCI√ìN =====
        description = None
        description_header = soup.find("h2", string="Job Description")

        if description_header:
            description_block = description_header.find_next("div")
            if description_block:
                description = description_block.get_text(
                    separator="\n", strip=True
                )

        # ===== JOB DETAILS =====
        details_text = soup.get_text(" ", strip=True)

        seniority = "Junior" if "Junior" in details_text else None
        contract_type = "Permanent" if "Permanent" in details_text else None

        # ===== ACTUALIZAR RAW JOB =====
        raw_job["description"] = description
        raw_job["seniority"] = seniority
        raw_job["contract_type"] = contract_type

        return raw_job

    def scrape_jobs(self) -> list:
        """
        FASE LISTADO: Extraer trabajos de ArtStation con filtrado

        Returns:
            Lista de diccionarios con informaci√≥n de trabajos
        """
        print("\n" + "=" * 70)
        print("SCRAPER DE ARTSTATION - CHARACTER ARTIST FOCUS")
        print("=" * 70)

        jobs = []

        try:
            print("\n‚Üí Navegando a ArtStation Jobs...")
            success = self.navigate_to(self.base_url)

            if not success:
                print("‚úó No se pudo cargar la p√°gina")
                return jobs

            print("‚Üí Esperando carga completa de trabajos...")
            time.sleep(4)

            print("‚Üí Haciendo scroll para cargar m√°s trabajos...")
            self.scroll_page(times=3)

            # Capturar screenshot
            screenshot_path = "../../research/platform_tests/artstation_playwright.png"
            self.screenshot(screenshot_path)
            print(f"‚úì Screenshot guardado")

            # Obtener HTML
            print("‚Üí Extrayendo HTML completo...")
            html = self.get_html()
            soup = self.parse_html(html)

            # Guardar HTML para an√°lisis
            html_path = "../../research/platform_tests/artstation_playwright.html"
            with open(html_path, "w", encoding="utf-8") as f:
                f.write(html)
            print(f"‚úì HTML guardado en: {html_path}")

            # BUSCAR TRABAJOS
            print("\n‚Üí Buscando trabajos con selectores identificados...")

            job_elements = soup.select('.job-grid-item')

            if not job_elements:
                print("‚ö†Ô∏è  No se encontr√≥ .job-grid-item, intentando selectores alternativos...")
                job_elements = (
                        soup.select('[class*="job-grid"]') or
                        soup.select('article') or
                        soup.select('[class*="job"]')
                )

            if not job_elements:
                print("‚ùå No se encontraron trabajos")
                return [{
                    "platform": "ArtStation",
                    "status": "NO_JOBS_FOUND",
                    "message": "Revisar HTML y selectores",
                    "html_file": html_path,
                    "screenshot": screenshot_path,
                    "timestamp": datetime.now().isoformat(),
                }]

            print(f"‚úÖ Encontrados {len(job_elements)} trabajos")

            # EXTRAER DATOS
            print("\n‚Üí Extrayendo informaci√≥n de trabajos...")

            for i, job_elem in enumerate(job_elements):
                try:
                    # T√≠tulo
                    title_elem = (
                            job_elem.select_one('.job-grid-item-title-holder') or
                            job_elem.select_one('h2') or
                            job_elem.select_one('h3') or
                            job_elem.select_one('[class*="title"]')
                    )

                    # Empresa
                    company_elem = (
                            job_elem.select_one('.job-grid-item-company') or
                            job_elem.select_one('[class*="company"]')
                    )

                    # Info adicional
                    info_elem = job_elem.select_one('.job-grid-item-info')
                    location_info = info_elem.get_text(strip=True) if info_elem else 'N/A'

                    # Link al trabajo
                    link_elem = job_elem.select_one('a[href*="/jobs/"]')

                    # Logo
                    logo_elem = job_elem.select_one('.job-grid-item-logo img')
                    company_logo = logo_elem.get('src', None) if logo_elem else None

                    # Extraer textos
                    title = title_elem.get_text(strip=True) if title_elem else 'N/A'
                    company = company_elem.get_text(strip=True) if company_elem else 'N/A'
                    url = link_elem.get('href', 'N/A') if link_elem else 'N/A'

                    # Construir URL completa
                    if url.startswith('/jobs/'):
                        url = f"https://www.artstation.com{url}"

                    # APLICAR FILTROS
                    is_character = self._is_character_artist(title)
                    is_entry = self._is_entry_level(title, location_info)
                    experience = self._extract_experience_years(location_info)

                    # Construir objeto job
                    job = {
                        'platform': 'ArtStation',
                        'title': title,
                        'company': company,
                        'location_info': location_info,
                        'url': url,
                        'company_logo': company_logo,
                        'experience_required': experience,
                        'scraped_at': datetime.now().isoformat(),
                        'is_character_artist': is_character,
                        'is_entry_level': is_entry,
                        'relevance_score': self._calculate_relevance(is_character, is_entry)
                    }

                    jobs.append(job)

                    # Mostrar con indicadores visuales
                    if is_character:
                        level = "üü¢ ENTRY" if is_entry else "üîµ MID/SR"
                        print(f"  [{i+1:2d}] {level} | {title[:50]}")
                        print(f"       ‚îî‚îÄ {company}")
                    else:
                        print(f"  [{i+1:2d}] ‚ö™ OTHER | {title[:50]}")

                except Exception as e:
                    print(f"  ‚úó Error en trabajo {i+1}: {e}")
                    continue

            print(f"\n‚úÖ Extracci√≥n completada: {len(jobs)} trabajos procesados")
            return jobs

        except Exception as e:
            print(f"\n‚ùå ERROR GENERAL: {e}")
            import traceback
            traceback.print_exc()
            return jobs


def main():
    """Funci√≥n principal con integraci√≥n Supabase"""
    print("\n" + "=" * 70)
    print("ARTSTATION SCRAPER - CHARACTER ARTIST ENTRY LEVEL")
    print("=" * 70)
    print(f"Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"Objetivo: Encontrar posiciones Character Artist (Entry Level)")
    print(f"Base de datos: {'‚úì Supabase ACTIVO' if DB_AVAILABLE else '‚úó Desactivada'}")
    print("=" * 70)

    # Ejecutar scraper
    with ArtStationScraper(headless=False) as scraper:
        jobs = scraper.scrape_jobs()

    # CLASIFICAR RESULTADOS
    print("\n" + "=" * 70)
    print("CLASIFICANDO RESULTADOS")
    print("=" * 70)

    character_jobs = [j for j in jobs if j.get('is_character_artist', False)]
    entry_jobs = [j for j in character_jobs if j.get('is_entry_level', False)]
    mid_senior_jobs = [j for j in character_jobs if not j.get('is_entry_level', False)]
    other_jobs = [j for j in jobs if not j.get('is_character_artist', False)]

    print(f"\nüìä ESTAD√çSTICAS:")
    print(f"  Total scraped: {len(jobs)}")
    print(f"  Character Artist: {len(character_jobs)} ({len(character_jobs)/len(jobs)*100 if jobs else 0:.1f}%)")
    print(f"    ‚îî‚îÄ üü¢ Entry Level: {len(entry_jobs)}")
    print(f"    ‚îî‚îÄ üîµ Mid/Senior: {len(mid_senior_jobs)}")
    print(f"  Other roles: {len(other_jobs)}")

    # GUARDAR EN SUPABASE
    if DB_AVAILABLE and jobs:
        print("\n" + "=" * 70)
        print("üíæ GUARDANDO EN SUPABASE")
        print("=" * 70)

        db = SessionLocal()
        saved_count = 0
        updated_count = 0
        error_count = 0

        try:
            for job in jobs:
                # Enriquecer con descripci√≥n (opcional, comentar si es muy lento)
                # detailed_job = scraper.scrape_job_detail(job)
                detailed_job = job  # Sin detail por ahora

                # Convertir a formato DB
                db_job = ArtStationScraper(headless=True)._to_db_job(detailed_job)

                # Guardar
                if save_job(db, db_job):
                    saved_count += 1
                else:
                    error_count += 1

        except Exception as e:
            print(f"\n‚ùå Error al guardar en BD: {e}")
        finally:
            db.close()

        print(f"\n‚úÖ Persistencia completada:")
        print(f"   Guardados: {saved_count}")
        print(f"   Errores: {error_count}")

    # GUARDAR JSON
    character_jobs_sorted = sorted(
        character_jobs,
        key=lambda x: x.get('relevance_score', 0),
        reverse=True
    )

    result = {
        'platform': 'ArtStation',
        'scrape_date': datetime.now().isoformat(),
        'filters_applied': {
            'target_role': 'Character Artist',
            'target_level': 'Entry Level (preferred)',
            'keywords_used': [
                'character artist', '3d character', 'character modeler',
                'junior', 'entry level', 'associate'
            ]
        },
        'summary': {
            'total_scraped': len(jobs),
            'character_artist_total': len(character_jobs),
            'entry_level_count': len(entry_jobs),
            'mid_senior_count': len(mid_senior_jobs),
            'other_roles_count': len(other_jobs),
        },
        'jobs': {
            'top_matches_entry_level': entry_jobs,
            'character_artist_mid_senior': mid_senior_jobs,
            'all_character_artist': character_jobs_sorted,
            'all_jobs': jobs
        }
    }

    result_file = "../../research/platform_tests/artstation_playwright_result.json"
    with open(result_file, "w", encoding="utf-8") as f:
        json.dump(result, f, indent=2, ensure_ascii=False)

    print(f"\nüíæ Resultados guardados en: {result_file}")

    # MOSTRAR TOP RESULTADOS
    if entry_jobs:
        print("\n" + "=" * 70)
        print("üéØ TOP POSICIONES ENTRY LEVEL ENCONTRADAS")
        print("=" * 70)

        for i, job in enumerate(entry_jobs[:5], 1):
            print(f"\n{i}. {job['title']}")
            print(f"   üè¢ Empresa: {job['company']}")
            print(f"   üìç Info: {job['location_info']}")
            print(f"   üìÖ Experiencia: {job['experience_required']}")
            print(f"   üîó URL: {job['url']}")
            print(f"   ‚≠ê Relevancia: {job['relevance_score']}/10")

    # Archivos generados
    print("\n" + "=" * 70)
    print("üìÅ ARCHIVOS GENERADOS")
    print("=" * 70)
    print(f"  1. artstation_playwright.html (HTML completo)")
    print(f"  2. artstation_playwright.png (Screenshot)")
    print(f"  3. artstation_playwright_result.json (Datos estructurados)")
    if DB_AVAILABLE:
        print(f"  4. ‚úì Datos guardados en Supabase")
    print("=" * 70 + "\n")


if __name__ == "__main__":
    main()